---
title: 'Cla&Clu: Myocardial infarction'
author: "Tolga Tabanli"
date: "2025-10-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup and Data

```{r}
library(tidyverse)
library(tidymodels)

data <- read_csv("data/breast-cancer-wisconsin.data",
                 na = "?",
                 col_names = c("id_number", "clump_thickness",
                               "uniformity_of_cell_size", "uniformity_of_cell_shape",
                               "marginal_adhesion", "single_epithelial_cell_size", 
                               "bare_nuclei", "bland_chromatin", "normal_nucleoli",
                               "mitoses", "class"))
```

### Visualize missingness

Use `vism` naniar

```{r}
library(naniar)

vis_miss(data)
```

### Clustering

We are going to apply PCA to reduce the dimensionality of the data to
two main axes that capture the most variance.
For the clustering we are going to use all the data, since
we are not using the outcome or predict anything to test afterwards.
Your task is:

1. First create a recipe with formula `~ .`. This just means that we do not focus on any outcome variable yet and use all the variables.
2. We have an id column named **id_number** and we want to ignore the outcome.
Update their roles accordingly.
3. Normalize all the predictors.
4. Impute missing values using kNN.
5. Apply PCA to get 4 principal components.
6. *Prepare* the recipe and *bake* on the data. The components will be named as PC1, PC2, etc.
7. Plot the principal components in pairs using the outcome as the color of the points.
**Hint**: We can plot principal components pairwise using `ggpairs()` from the
GGally package:

```{r, eval = F}
pc_axes <- paste0("PC", 1:4)

library(GGally)
ggpairs(
  data = YOUR_DATA_WITH_PC,
  columns = pc_axes,
  mapping = aes(color = as.factor(TARGET))
)
```



```{r}
library(GGally)
recipe <- recipe(~ ., data = data) %>%
  update_role(id_number, new_role = "id") %>%
  update_role(class, new_role = "ignore") %>%
  step_normalize(all_predictors()) %>%
  step_impute_knn(all_predictors(), neighbors = 10) %>%
  step_pca(all_numeric_predictors(), num_comp = 4)

baked_data <- prep(recipe) %>%
  bake(new_data = data)

pc_axes <- paste0("PC", 1:4)
ggpairs(
  baked_data,
  columns = pc_axes,
  mapping = aes(color = as.factor(class))
)
```


### Prediction model

Split the data into training (80 %) and test set (20 %).
Use the same preprocessing but specify the formula
with the target and predictors.
Train a logistic regression model and a random forest.
In the random forest, tune for all the three parameters.
After finalizing the models, report the accuracy, precision and roc_auc metrics
on the test set.

```{r}

```


